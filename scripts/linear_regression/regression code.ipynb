{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3) Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After constructing the training dataset, the next step is to train a regression model and use it for prediction.\n",
    "Let us start with the simplest one which is linear regression. As you know, there are two ways to solve the\n",
    "linear regression problem: implementing the gradient descent algorithm and using the closed form\n",
    "solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## MODIFICATIONS BY ALRH: ##\n",
    "import os\n",
    "\n",
    "\n",
    "## MODIFICATIONS BY ALRH: ##data_collection\n",
    "#Note: This might not work if script is run virtually.\n",
    "from scripts.data_collection.data_generator import data_collection, plot_feature_correlation, split_data\n",
    "### END ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent (X, y, learning_rate, iterations):\n",
    "    m = len(y)\n",
    "    theta = np.random.randn(X.shape[1], 1)  # initialization\n",
    "    history = []\n",
    "    history.append(theta)\n",
    "    for i in range(iterations):\n",
    "        # Note (ALRH): I fixed the gradient calculation.\n",
    "        # Before, it was:\n",
    "        #old_gradient = 2/m * X.T.dot(X.dot(theta)-y)\n",
    "        # Now, it is:\n",
    "        gradient = -(2/m) * X.T.dot(y-X.dot(theta))\n",
    "        theta = theta - learning_rate * gradient\n",
    "        history.append(theta)\n",
    "    return theta\n",
    "\n",
    "def closed_form (X, y):\n",
    "    return np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.1) Please show that these two methods end up with the same solution. For simplicity, you can start\n",
    "with the low number of samples, e.g. 100 datapoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent solution: [[-1029.86571743]\n",
      " [  669.55133209]]\n",
      "Closed form solution:  [[-1177.76736458]\n",
      " [  704.42702575]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## MODIFIED BY ALRH ##\n",
    "# load training data\n",
    "path_to_data = os.path.join(os.getcwd(), 'data')\n",
    "trainingdata = data_collection(path_to_data)\n",
    "#trainingdata = pd.read_csv ('trainingdata.csv')\n",
    "\n",
    "# load split data from training data;\n",
    "X_train, X_test, y_train, y_test = split_data(df=trainingdata.head(100), features=['mean_wind_speed'], targets=['Kalby_AP'])\n",
    "#X_train = trainingdata.iloc[:, :-1].values  # features\n",
    "#y_train = trainingdata.iloc[:, -1].values.reshape(-1, 1)  # target, column vector\n",
    "### END ###\n",
    "\n",
    "X_train_bias = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "\n",
    "# learning rate and iteration\n",
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "# solution\n",
    "gradientdescent_solution = gradient_descent(X_train_bias, y_train, learning_rate, iterations)\n",
    "closedform_solution = closed_form (X_train_bias, y_train)\n",
    "\n",
    "# print solutions\n",
    "print (\"Gradient descent solution:\", gradientdescent_solution)\n",
    "print (\"Closed form solution: \", closedform_solution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.2) Please increase the number of samples to improve the accuracy of prediction and only use the\n",
    "closed form solution to find the optimal regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed form solution with more samples: [[-424.88023757]\n",
      " [ 421.38314455]]\n"
     ]
    }
   ],
   "source": [
    "#trainingdatalarger = pd.read_csv('trainingdatalarger.csv')\n",
    "#testingdatalarger = pd.read_csv('testingdatalarger.csv')\n",
    "# load split data from training data;\n",
    "# Run with 1000 data points.\n",
    "X_train, X_test, y_train, y_test = split_data(df=trainingdata.head(1000), features=['mean_wind_speed'], targets=['Kalby_AP'])\n",
    "\n",
    "\n",
    "#X_train_largerset = trainingdatalarger.iloc[:, :-1].values  \n",
    "#y_train_largerset = trainingdatalarger.iloc[:, -1].values.reshape(-1, 1)\n",
    "X_train_largerset_bias = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "\n",
    "closedform_solution = closed_form(X_train_largerset_bias, y_train)\n",
    "\n",
    "print(\"Closed form solution with more samples:\", closedform_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.3) Verify your model using the testing dataset and appropriate evaluation metrics (e.g., Root mean\n",
    "squared error, Mean absolute error, R-squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared error: 1059.4397610541346\n",
      "Mean absolute error: 709.7030241501694\n",
      "R-squared: 0.5973426010553808\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "#load testing data\n",
    "path_to_data = os.path.join(os.getcwd(), 'data')\n",
    "trainingdata = data_collection(path_to_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(df=trainingdata, features=['mean_wind_speed'], targets=['Kalby_AP'])\n",
    "\n",
    "X_test_bias = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "closedform_solution = closed_form(X_test_bias, y_test)\n",
    "\n",
    "y_predicted = X_test_bias.dot(closedform_solution)\n",
    "\n",
    "# evaluation metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_predicted))\n",
    "mae = mean_absolute_error(y_test, y_predicted)\n",
    "r2 = r2_score(y_test, y_predicted)\n",
    "\n",
    "print(\"Root mean squared error:\", rmse)\n",
    "print(\"Mean absolute error:\", mae)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
